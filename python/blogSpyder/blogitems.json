## 基础知识

无论是自然语言还是编程语言， 我们站在语言学家的角度如何来描述一种语言呢？ 比较常用的方法有三种：  
1\. 穷举法， 就是列出该语言中所有的句子。  
2\. 文法描述： 语言中的每个句子都是通过严格的规则构造出来的。  
3\. 自动机： 随便给个句子，判断该句子是否是该语言中的句子。  
这三种方法都能描述一种语言， 显然第一种仅仅使用规模比较小的语言，自然语言，无论是英语还是汉语都没法穷举。
实际在自然语言处理中都是结合了文法描述和自动机两者的长处。

## 形式语言

用来精确的描述语言及其结构的手段。那么如何来描述特定的语言（包括自然语言和人工语言）呢 ， 就引出了下面的形式语言， 又叫形式文法；

## 形式语法的定义

形式语法是个四元组：G = (N,Σ,R,S) ， 其中N 为非终结符的有限集合，Σ是终结符号的有限集合。还有一个词汇表的概念， 我们表示成V，有V = N
∪ Σ ，R = {α-&gt;β} 其中α， β ∪ V ， 并且α 中至少含有一个非终结符。

## 形式语法的类型

Chomsky的语法理论中， 文法分为4中类型， 分别是:3型文法-正则文法， 2型文法-上下文无关文法，1型文法-上下文相关文法和0型文法-无约束文法。
下面就该说上下文无关语法了：

## 上下文无关语法（CFG）

定义：  
G = (N,Σ,R,S) where:  
N is a set of non-terminal symbols  
Σ is a set of terminal symbols I  
R is a set of rules of the form X → Y1Y2 …Yn for n ≥ 0, X ∈ N, Yi ∈ (N ∪Σ)  
S ∈ N is a distinguished start symbol  
上下文无关语法要求，规则R中所有规则要求满足如下形式：A-&gt;α ,A∪N , α∪(N ∪ Σ);  
什么意思呢， 就是在CFG中，所有的替换，仅仅依赖于规则，而跟规则所处的上下文环境无关。 相应的肯定有上下文相关语法。

## 上下文相关文法：（content-sensitive grammar）

如果文法G的规则R中所有规则满足如下形式：αAβ-&gt;αγβ,其中A∪N , α，γ，β∪(N ∪
Σ);并且γ中至少包含一个终端字符。这样的是上下文相关文法（CSG）；  
什么意思呢， 就是所有的替换， 不仅仅要依赖于规则，还要参考规则所在的上下文环境。

## 要解决的问题：词性标注；

用监督学习的方法来解决词性标注问题；  
如句子“The dog laugh”我们知道其对应的词性序列为“P N V ”其中P为介词 ， N为名词，
V为动词；那我们如何用机器学习的方法来解决这个问题呢? 我们可以用监督学习的方法来解决这个问题，本文大概描述描述这个过程。  
对于输入的句子“ The dog laugh”中每个词都有可能是{P ，N ，V ，S} 四种词性中的一种，其中S
为结束标志。那么该输入语句对应的可能的词性序列就是 3^3 = 27种，第一个3 输入序列的长度，指数位置的3 是每个词可能的词性，在此例子中有3种可能（s
是结束标志，仅仅允许出现在最后一个位置）。  
这27 种词性序列中，每一个序列都对应其出现的概率，那么出现概率最高的那个序列就是我们想要得到的序列。也就是求p(y1,y2,y3|x1,x2,x3)  
使用的模型，隐马尔可夫模型；

由生成模型可知：p(y1,y2,…yn|x1,x2,…xn)的最大值就是p(x1…xn,y1,y2,…,yn)的最大值；  
![这里写图片描述](http://img.blog.csdn.net/20160402214622827)  
上式的推导过程大概如下：  
p(x1,x2…xn,y1,y2…yn)= p(x1,x2…xn|y1,y2…yn) * p(y1,y2,..yn)  
其中等号右边第二项：p(y1,y2,..yn)由于链式法则和马尔科夫假设可以写成：Πq(yi|yi-2,yi-1)  
其中等号右边第一项：p(x1,x2…xn|y1,y2…yn) 由于xi 相互独立， 又由于xi 的概率仅仅跟i位置上的标签（P ，N ，V
，S）相关，跟i-1,i+1 位置的标签不相关，所以该项可以写成：  
p(x1,x2…xn|y1,y2…yn)= Πp(xi| y1,y2…yn)= Πp(xi|yi)  
合并起来就得到了截图中的表达式了。

## 解决计算量的问题，维特比算法；

式2.5 所能计算的是针对某个输入序列，对应一个特定的输出标记序列的概率。例如输入序列为“The dog laugh” 对应的输出标记序列为“P N V
S” 的概率。 但是对于一个输出入序列可能对应的标记序列实在是太多。长度为n的输入序列可能的输出序列就有n^s ，其中s为词性序列的长度，此例中为3。

维特比算法本质上是动态规划算法，这儿摘取《数学之美》相关章节来说明动态规划的思想：  
![这里写图片描述](http://img.blog.csdn.net/20160402214727421)  
我们的问题难以解决是因为有n^s¬个概率要计算， 而动态规划的思想就是仅关注当前这一步， 让当前的值取到最优。
这样的话我们当前这一步需要比较的概率就仅仅有s个值了。  
维特比算法中有以下定义：  
![这里写图片描述](http://img.blog.csdn.net/20160402214747718)  
π（k,u,v）表示的是第k个位置的词对应的词性是v，并且第k-1个词的位置对应的词性是u的最大概率。  
下面给出个例子，并给出求解过程， 这个例子来自MichaelCollins 的课程。  
![这里写图片描述](http://img.blog.csdn.net/20160402214823234)  
求解过程如下：  
![这里写图片描述](http://img.blog.csdn.net/20160402214850828)
